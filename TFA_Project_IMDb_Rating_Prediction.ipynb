{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING USEFUL PYTHON LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing useful libraries in Python\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas, xgboost, numpy, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/07/be624c7e0a63b080a76b7f6faf417eecdc5f6480f6a740a8bcf8991bce0b/tensorflow-1.12.0-cp36-cp36m-macosx_10_11_x86_64.whl (62.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 62.0MB 410kB/s ta 0:00:011    32% |██████████▌                     | 20.3MB 6.7MB/s eta 0:00:07    38% |████████████▍                   | 24.1MB 8.6MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.5)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.6)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/27/133f225035b9539f2dcfebcdf9a69ff0152f56e0120160ec5c972ea7deb9/protobuf-3.6.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 4.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/d0/65fe48383146199f16dbd5999ef226b87bce63ad5cd73c840cf722637969/tensorboard-1.12.0-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/63/f505d2d4c21db849cf80bad517f0065a30be6b006b0a5637f1b95584a305/absl-py-0.6.1.tar.gz (94kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 10.9MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.14.3)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/1e/61db56d478ca2e9a8f58172f619c08db5010c4fd2897d2baaff0d9c4f8a3/grpcio-1.16.1-cp36-cp36m-macosx_10_7_intel.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 5.8MB/s eta 0:00:01   21% |███████                         | 440kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: h5py in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Requirement already satisfied: setuptools in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.6.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 8.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /Users/paramawasthi/anaconda3/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Building wheels for collected packages: gast, absl-py, termcolor\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/paramawasthi/Library/Caches/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/paramawasthi/Library/Caches/pip/wheels/18/ea/5e/e36e1b8739e78cd2eba0a08fdc602c2b16a4b263912af8cb64\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/paramawasthi/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built gast absl-py termcolor\n",
      "Installing collected packages: gast, protobuf, astor, markdown, grpcio, tensorboard, absl-py, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.6.1 astor-0.7.1 gast-0.2.0 grpcio-1.16.1 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.0 tensorflow-1.12.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "n_jobs_cnt = multiprocessing.cpu_count()-1\n",
    "n_jobs_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING THE DATA AND DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train review text and label files to dataframe\n",
    "import pandas as pd\n",
    "df_test_text = pd.read_table('imdb_test_text.txt', delim_whitespace=False, names=('A'))\n",
    "df_test_labels = pd.read_table('imdb_test_labels.txt', delim_whitespace=False, names=('B'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging them on index\n",
    "df_test=df_test_text.join(df_test_labels, how='outer')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing test review text and label files to dataframe\n",
    "df_train_text = pd.read_table('imdb_train_text.txt', delim_whitespace=False, names=('A'))\n",
    "df_train_labels = pd.read_table('imdb_train_labels.txt', delim_whitespace=False, names=('B'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging them on index\n",
    "df_train=df_train_text.join(df_train_labels, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I loved this movie since I was 7 and I saw it ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First things first, Edison Chen did a fantasti...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once again, I was browsing through the discoun...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a gem, a real piece of Americana for a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While I had wanted to se this film since the f...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_text  review_label\n",
       "S.No.                                                                 \n",
       "0      I loved this movie since I was 7 and I saw it ...            10\n",
       "1      First things first, Edison Chen did a fantasti...             8\n",
       "2      Once again, I was browsing through the discoun...             7\n",
       "3      This is a gem, a real piece of Americana for a...             8\n",
       "4      While I had wanted to se this film since the f...             8"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing column name to review_text and review_label\n",
    "df_train=df_train.rename(columns={'A':'review_text',\n",
    "                          'B':'review_label'})\n",
    "df_train.index.name='S.No.'\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not really sure what to make of this movie. ve...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you enjoyed films like Pulp Fiction, Reserv...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, here's the deal. There's this American p...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The BBC surpassed themselves with the boundari...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Victor Mature, as a barely civilized and mostl...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I remember this film as the other person that ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I first saw Heimat 2 on BBC2 in the 90's when ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A rich experience is to be gained from watchin...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The second (not animated) movie about the only...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I first saw this in the theater in 1969 when I...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_text  review_label\n",
       "S.No.                                                                 \n",
       "0      not really sure what to make of this movie. ve...             7\n",
       "1      If you enjoyed films like Pulp Fiction, Reserv...            10\n",
       "2      Okay, here's the deal. There's this American p...            10\n",
       "3      The BBC surpassed themselves with the boundari...            10\n",
       "4      Victor Mature, as a barely civilized and mostl...             8\n",
       "5      I remember this film as the other person that ...            10\n",
       "6      I first saw Heimat 2 on BBC2 in the 90's when ...            10\n",
       "7      A rich experience is to be gained from watchin...             9\n",
       "8      The second (not animated) movie about the only...             9\n",
       "9      I first saw this in the theater in 1969 when I...            10"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing column name to review_text and review_label\n",
    "df_test=df_test.rename(columns={'A':'review_text',\n",
    "                          'B':'review_label'})\n",
    "df_test.index.name='S.No.'\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenating two datasets\n",
    "df_all = pd.concat(objs=[df_train,\n",
    "                         df_test],\n",
    "                   axis=0)\n",
    "df_all.reset_index(inplace=True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I loved this movie since I was 7 and I saw it ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First things first, Edison Chen did a fantasti...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once again, I was browsing through the discoun...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a gem, a real piece of Americana for a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While I had wanted to se this film since the f...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  review_label\n",
       "0  I loved this movie since I was 7 and I saw it ...            10\n",
       "1  First things first, Edison Chen did a fantasti...             8\n",
       "2  Once again, I was browsing through the discoun...             7\n",
       "3  This is a gem, a real piece of Americana for a...             8\n",
       "4  While I had wanted to se this film since the f...             8"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droping serial_no column\n",
    "df_all.drop(labels=['S.No.'],\n",
    "            inplace=True,\n",
    "            axis=1)\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARING THE DATASETS FOR MODEL FITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000,), (10000,), (40000,), (10000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the dataset into test and train for implementing ML models \n",
    "#mention a random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_all['review_text'],\n",
    "                                                    df_all['review_label'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3,  8,  2,  1, 10,  4,  7,  9]),\n",
       " array([ 9, 10,  8,  3,  1,  4,  7,  2]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking unique labels present in test and train data\n",
    "y_train.unique(), y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000,), (10000,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encode the target variable\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_en = encoder.fit_transform(y_train)\n",
    "y_test_en = encoder.transform(y_test)\n",
    "\n",
    "y_train_en.shape, y_test_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to fit different ML models one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, y_test_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We will now try to run fit our data through various ML models. The models will be implemented after vectorization, mainky tf-idf or count-vectorizer or both. \n",
    "We have used following models with different vectorization parameters to achieve highest accuracy:\n",
    "\n",
    "1. Naive Bayes\n",
    "2. Neural Network\n",
    "3. Random Forest Classifier\n",
    "4. XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING TF-IDF VECTORIZER AND COUNT VECTORIZER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was done in multiple iterations with different parameter values to achieve maximum accuracy on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-DIF ON WORD LEVEL, NGRAM LEVEL, CHARS LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "%%time\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             max_features=2000,\n",
    "                             min_df=0.01, \n",
    "                             max_df=0.95)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', \n",
    "                                   token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   max_features=2400,\n",
    "                                   min_df=0.01, \n",
    "                                   max_df=0.95)\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', \n",
    "                                         token_pattern=r'\\w{1,}', \n",
    "                                         ngram_range=(2,5), \n",
    "                                         max_features=10000,\n",
    "                                         min_df=0.01, \n",
    "                                         max_df=0.95)\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tranfroming the test data with TF-IDF Vectorizer\n",
    "%%time\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "X_test_tfidf_ngram = tfidf_vect_ngram.transform(X_test)\n",
    "X_test_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting y labels into series object for model fitting\n",
    "y_test_en = pd.Series(y_test_en)\n",
    "y_train_en = pd.Series(y_train_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating count vectorizer object\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000, min_df=0.02, max_df=0.95)\n",
    "count_vect.fit(df_all['review_text'])\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "X_train_count =  count_vect.fit_transform(X_train)\n",
    "X_test_count =  count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FITTING THE MODEL  AND FINDING ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF VECTORIZER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF:  0.395\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf, y_train_en, X_test_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors:  0.3769\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram , y_train_en, X_test_tfidf_ngram )\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors:  0.3895\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram_chars, y_train_en, X_test_tfidf_ngram_chars )\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.389\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectorizer\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), X_train_count, y_train_en, X_test_count)\n",
    "print(\"RF, Count Vectors: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING TF-IDF VECTORIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was done in multiple iterations with different parameter values to achieve maximum accuracy on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "%%time\n",
    "tfidf_vec`t = TfidfVectorizer(analyzer='word', \n",
    "                             token_pattern=r'\\w{1,}', \n",
    "                             max_features=3000,\n",
    "                             min_df=0.01, \n",
    "                             max_df=0.95)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "%%time\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', \n",
    "                                   token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(2,3), \n",
    "                                   max_features=1500,\n",
    "                                   min_df=0.01, \n",
    "                                   max_df=0.95)\n",
    "X_train_tfidf_ngram = tfidf_vect_ngram.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "%%time\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', \n",
    "                                         token_pattern=r'\\w{1,}', \n",
    "                                         ngram_range=(2,4), \n",
    "                                         max_features=5600,\n",
    "                                         min_df=0.01, \n",
    "                                         max_df=0.95)\n",
    "X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranfroming the test data with TF-IDF Vectorizer\n",
    "%%time\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "X_test_tfidf_ngram = tfidf_vect_ngram.transform(X_test)\n",
    "X_test_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model architecture to fit Neural Network\n",
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FITTING THE MODEL  AND FINDING ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 4s 108us/step - loss: -36.9859\n",
      "NN, WordLevel TF-IDF:  0.2015\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "classifier = create_model_architecture(X_train_tfidf.shape[1])\n",
    "accuracy = train_model(classifier, X_train_tfidf, y_train_en, X_test_tfidf, is_neural_net=True)\n",
    "print (\"NN, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 5s 113us/step - loss: -36.3661\n",
      "NN, N-Gram Vectors:  0.2015\n"
     ]
    }
   ],
   "source": [
    "# ngram level tf-idf\n",
    "classifier = create_model_architecture(X_train_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, X_train_tfidf_ngram , y_train_en, X_test_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 32s 794us/step - loss: -38.8145\n",
      "NN, CharLevel Vectors:  0.2015\n"
     ]
    }
   ],
   "source": [
    "#ngram_chars level tf-idf\n",
    "classifier = create_model_architecture( X_train_tfidf_ngram_chars.shape[1])\n",
    "accuracy = train_model(classifier, X_train_tfidf_ngram_chars, y_train_en, X_test_tfidf_ngram_chars,is_neural_net=True )\n",
    "print (\"NN, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF vectorizer to find maximum accuracy in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors for max features= 500 : 0.2955\n",
      "RF, Count Vectors for max features= 1000 : 0.3334\n",
      "RF, Count Vectors for max features= 1500 : 0.3191\n",
      "RF, Count Vectors for max features= 2000 : 0.3124\n",
      "RF, Count Vectors for max features= 2500 : 0.3217\n",
      "RF, Count Vectors for max features= 3000 : 0.3182\n",
      "RF, Count Vectors for max features= 3500 : 0.3219\n",
      "RF, Count Vectors for max features= 4000 : 0.3218\n",
      "RF, Count Vectors for max features= 4500 : 0.3152\n",
      "RF, Count Vectors for max features= 5000 : 0.3221\n"
     ]
    }
   ],
   "source": [
    "i=500\n",
    "while i < 5001:\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=i, min_df=0.01, max_df=0.95)\n",
    "    # # transform the training and validation data using count vectorizer object\n",
    "    X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "    accuracy = train_model(ensemble.RandomForestClassifier(), X_train_tfidf, y_train_en, X_test_tfidf)\n",
    "    print(\"RF, Count Vectors for max features=\",i,\":\",accuracy)\n",
    "    i=i+500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing TF-IDF with 1000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000, min_df=0.01, max_df=0.95\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranfroming the test data with TF-IDF Vectorizer \n",
    "X_test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 396 ms, total: 16.8 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000, min_df=0.01, max_df=0.95)\n",
    "\n",
    "count_vect.fit(df_all['review_text'])\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(X_train)\n",
    "xvalid_count =  count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FITTING THE MODEL  AND FINDING ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF: 0.3212\n"
     ]
    }
   ],
   "source": [
    "# word-level tf-idf\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), X_train_tfidf, y_train_en, X_test_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.3244\n"
     ]
    }
   ],
   "source": [
    "#  RF on Count Vectorizer\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, y_train_en, xvalid_count)\n",
    "print(\"RF, Count Vectors: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. XG BOOST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a CPU count for XGBoost Model to run at max processor efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "n_jobs_cnt = multiprocessing.cpu_count()-1\n",
    "n_jobs_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining XGBoost Classifier and using GridSearch with specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgboost.XGBClassifier(n_estimators=300,\n",
    "                                n_jobs=n_jobs_cnt)\n",
    "param_grid_xgb = {'max_depth':[3,5],\n",
    "                  'min_child_weight':[1,2],\n",
    "                  'learning_rate':[0.05,0.1]}\n",
    "grid_xgb = GridSearchCV(estimator=xgb_clf,\n",
    "                        param_grid=param_grid_xgb,\n",
    "                        verbose=10,\n",
    "                        return_train_score=False,\n",
    "                        scoring='accuracy',\n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the XG Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_xgb.fit(X=X_train_tfidf_ngram,\n",
    "             y=y_train_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best score, best parameter set and best estimator score out of the given set of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb.best_score_, grid_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb.best_estimator_.score(X_test_tfidf_ngram,\n",
    "                               y_test_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
